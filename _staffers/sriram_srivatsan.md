---
name: Sriram Srivatsan
role: AI
email: sriram.srivatsan@
website: https://www.linkedin.com/in/sriram-srivatsan-6b5024268/
photo: sriram.png
---
0. Test  12. Test  12,  That's  good. Thank  you,  my  friend. Yeah.  All  right. Oh,  lights.  Lights.  Okay. 1010.  Let's  get  started. Welcome  everybody  to  61. What?  Welcome  to  UC  Berkeley. Welcome  back  from  the  summer. I  hope  you  had  an  amazing  time. We're  so  excited  for  you. I  have  to  give  you  one  announcement. Hope  you  can  hear  me  okay. One  announcement,  which  is  you  got  to  wait till  one  A  gets  out  before  you  can  come  in. Otherwise, there's  just  a  log  jam  at  that  door. So  please  wait  till  they  flush  the  room. They'll  try  to  get  out  as  fast  as  they  can. Then  you  can  come  in. So  just  wait  for  them.  That's  just kind  of  a  fire  hazard,  you  thing. Alright?  61.  The  great  ideas in  computer  architecture. This  is  an  amazing  course. We're  so  happy  to  be  here. My  name  is  Dan  Garcia.  I'm  co  teaching. This  was  my  awesome  colleague,  Justin  Yakota. Let's  give  Justin  some  love. Whoo  beautiful.  We  always  start, we  actually  sometimes  start  before  1010. We'll  start  like  eight with  computing  in  the  news. This  is  the  computing  news  for  today, which  is  a  Janet  Jackson's song  that  I  had  cued  up, but  because  of  zoom  troubles  I couldn't  play  for  you  in  time. Could  crash  Windows  XP  laptops? And  this  is  a  very  funny  thing if  you  read  the  details  of  it. The  sound  frequency  in  her  song could  crash  a  model  laptop. Using,  playing  the  music  on  one  laptop causes  a  laptop  sitting  nearby  to  crash. How  about  that  for  a  fascinating  conversation about  security  and  making sure  you  don't  something  about  residence, frequency,  and  things.  It's  pretty  funny. Okay,  let's  jump  in. This  course  is  about  machine  structures. It's  called the  Great  Ideas  in  Computer  Architecture. I'm  going  to  do quite  a  bit  of  it  and  then  we'll pass  it  over  a  Justin, then  pass  them  back  to  me. So  let's  talk  about  machine  structures. 61,  despite  its  name, is  not  about  C  programming. It's  about  the  hardware  software  interface, one  of  the  most  important  interfaces, tractions  in  all  of  computing. What  does  a  program  need  to  know to  achieve  massive  performance? Languages  like  C  are  closer  to  the  metal, so  we  care  about  them. Unlike  languages  like  Snap, Python,  Java,  really  high, much  higher  level,  not  as  close  to  the  metal. You  can't  get  the  performance you  want  out  of  them. They're  a  little  bit  slower.  C  is really  super  low  level, so  we're  going  to  work  on  that. The  programmer  can  explicitly  harness underlying  hardware  parallelism  and that's  going  to  be  amazing  to  get. You're  gonna  have  a  performance  project. You're  gonna  be  like,  oh  my  gosh, I  can't  believe  I  made my  computer  run  this  fast. It's  gonna  be  super  fun.  Okay,  this is  Old  School  61. We  actually  made  a  fundamental  change  to  61. This  is  what  it looked  like  in  the  olden  days. Now  note,  this  photo  is fake,  just  so  you  know. But  it's  kind  of,  this  is  the  old  school 61  and  this  is  new  school  61. Ooh,  ooh,  you've  got  a  test. I  drove  my  testa  to  work. Today  you've  got  a,  a  drone  over  here. I  played  with  my  drone  this  weekend. This  is  the  future. The  network  has  edge  devices. You've  got  personal  mobile  devices. What  runs  all  this  stuff  like  what actually  runs all  those  personal  mobile  devices. This  a  warehouse  scale  computer. These  are  unbelievable  power  storage and  compute  power  boxes. These  are  huge  warehouses  where  you've  got so  many  CPU's  inside  of  those. Let's  take  a  look.  This  is, my  other  computer  is  a  data  center. This  is  a  data  center.  This  is  what runs  all  of  your  social  media  devices. What  allows  you  have  the  power  of  this? Well,  let's  allows  me  to  say,  here, watch  it,  What  time  is  it? Lets  see,  let's  see  if Siri's  going  to  be friendly  to  me  today  with  the  Wi  Fi. Give  me  something  on  it,  on it  and  it  has  no  Wifi. Syria  would  have  answered  if it  had  Wi  Fi  down  here  in the  basement  Data  center.  Pretty  amazing. This  is  again  the  old  school where  we  looked  at,  here  we  go. We  looked  at  a  single  computer. We  were  only  looking  at the  hardware,  software  interface for  a  single  computer. From  all  the  way  to the  top  where  you  basically  live, you  live  up  there,  you  live  in the  browser,  you  live  in  an  OS. You  probably  have  played a  little  bit  in  62  and  B  with  a  compiler, haven't  really  played  with  an  assembler, but  you  basically  compiled down  to  an  executable. This  is  the  hardware  software  interface  known as  the  obstruction  set  architecture. Like  any  abstraction  from  the  layer  above, like  kind  of  like  living  on  the  11th  floor of  a  New  York  high  rise  from  the  11th  floor, you  only  deal  with people  above  you  and  below  you. That's  all  you  ever  deal  with.  So  above  you, I  could  build  this  hardware  out  of anything  I  could  build  out  of  a  real  chip. I  could  be  Apple  and  make  it  another  chip. I  could  decide  to  run  it on  gerbils  and  spin  wheels  and  rubber  bands. I  could  still  do  that.  And  you  could  have a  running  computer  that  from the  point  of  view  above  the  line, you  wouldn't  even  know what's  below,  just  below. Slower,  pretty  cool  stuff. We're  gonna  learn  this  entire  stack. This  course  is  amazing. Only  in  this  course  do  you  finally learn  this  entire  stack from  hardware  to  software. The  hand  of  God  of software  touches  the  hand  of  man  of  hardware. It's  gonna  be  amazing. We're  gonna  have a  celebration  when  that  happens. It's  gonna  be,  we're  gonna  dance together.  It's  gonna  be  really  fun. You're  gonna  learn  about all  the  layers  below  it, a  processor  memory,  IO  systems is  lectures  on  each  one  of  these  things. Data  path  control,  digital  design, circuit  design,  transistors, all  the  way  down  to  fabrication, which  we'll  touch  about  just  for a  little  bit.  Pretty  exciting. The  whole  connection,  if  you've  always wondered  around  your  life how  it  really  worked, this  course  tells  you  how  it  really  works. Pretty  exciting. Here's  a  new  schools,  that's  old  school. We  do  old  school  and we  do  the  new  school  stuff.  Which  is  what? Well,  this  data  center  is a  warehouse  scale  computer  which drives  all  your  smart  devices. You  have  in  that  data  center, you've  got  a  server,  you've  got  a  rack, you  get  an  array  on  a  server. You  might  have  multiple  cores with  memory  and  input  output. You'll  talk  about  all  these  el,  you'll understand  every  picture, this  picture  I'm  going  to  show now  and  we're  showing  the  last  day  of  class. You'll  be  like,  I  can't  believe  all that  we've  learned  in  this  class. It's  going  to  be  amazing. Inside  that  compute  core, you  might  have  execution  units, functional  blocks,  main  memory. How  does  that  work? Well,  the  functional  blocks  A,  build  a  logic. You're  going  to  understand  everything all  the  way  down  to  that  shape, and  know  what  that  shape  actually  does. You'll  understand  everything  in this  course,  or  at  least  that's  the  goal. Now  we're  going  to  realize  that  if  you've  got a  warehouse  of  100,000  1  million  cores, 1  million  computers, what  can  you  do  with  those? Well,  you  can  harness  parallelism  to get  unbelievably  high  performance. When  you  click  on  Google, you  say  something  in  tenth  of  a second  as  a  response.  How  does  that  happen? This,  the  parallelism. So  these  are  all  the  parallelism  options. We've  added  this,  we  took the  old  school,  621, just  that  hardware  software stack,  and  we've  added  this. So  you're  going  to  get  all  of  that  in one  course.  It's  almost  too  much. It's  really  a  wonderful  overflowing, it's  a  potpourri  of information,  it's  amazing. Parallel  requests,  threads,  instructions, data  and  hardware  descriptions, all  of  which  are  happening  in parallel  at  the  same  time. All  these  will  go  into,  this is  just  a  very  high  level  overview. So  what  are the  great  ideas  of  computer  architecture? There  are six  great  ideas  of  computer  architecture. Here  they  are.  I'm  just  going  to  pause. Who  wants  to  take  a  selfie  here? Selfie,  Selfie  go  selfie,  this  is, it  is,  That's  the  picture  six  great  ideas. Computer  architecture,  abstraction, the  most  important  idea  of all  of  engineering  abstraction, Moore's  Law,  we'll  talk  about that  the  following  slides  describe  that. I'm  just  telling  what  they  are,  principle of  locality  or  memory. Hierarchy,  parallelism, performance,  measurement  and redundancy  or  improvement. And  then  dependability  via  redundancy. So  the  first  one,  abstraction levels  of  representation  interpretation, that's  all  the  layers  you  saw  that I  saw  in  the  old  school  picture. All  those  layers,  that's  all  abstraction. Here  you  have  a  piece  of  Python  code  where X  can  take  on  different  types, every  single  line  is  a  different  type. It's  pretty  cool,  anything can  be  a  Python  name. We  have  the  same  idea  when  we  look  at, I'll  show  this  picture  for  a  while. The  first  time  we've  ever  seen  this is  you  started  at  a  high  level. Language  like  C, probably  looks  like  Java  to  you. And  Java  look  very similar,  swapping  two  variables. I'm  just  swapping  that.  This  is how  we  do  that  in  assembly  language. You're  going  to  learn  assembly  language, it's  risk  five,  it's  awesome. We'll  talk  about  that. And  this  looks  like  great  to  you. You'll  understand  it  by  the  time we  get  through  those  lectures. This  one  for  one, every  single  line  you  see  maps to  32  bits.  That's  super  cool. So  now,  and  in  fact  it's also  really  amazing  is  you  can go  from  here  down  Polo, you  can  take  this  machine  code, assembly  code  and  make  it  machine  code, assembly  is  in  the  blue, machine  code  is  in  white. You  can  also  do  the  reverse way,  way,  way,  way. You  can  go  from  the  ones  and  zeros  back  to assembly  and  you  can  go  to  assembly  back  to C.  So  you  can  literally  get a  binary  piece  of  code, like  no  way  this  is  a  program. Yeah.  And  you  can  actually  figure  out, oh  my  god,  that  was  angry  birds. No  way.  I  mean, it'd  be  a  big  one,  but  it'd  still  be  amazing. You  could  actually  figure  that  out. Isn't  that  unbelievable? That's  pretty  cool  stuff.  Well  then you  got  to  run  this  on  something. That's  where  that  instruction set  architecture  comes  in. This  is  the  line between  hardware  and  software, and  below  that  we're going  to  build  hardware  that runs  exactly  those  ones  and  zeros. You're  fighting  like,  oh  my  gosh, I  know  what  that  one  means. And  that  one  means  do something  special  on  this  piece of  hardware  that  pause  for  it.  I  built. You  would  say  that  I  built. You  would  build  this. You're  gonna  build  the  hardware to  actually  run  these  ones  in  zero. It's  amazing  to  ground this  in  that  beautiful  way. Well,  how  does  this  memory  work? How  does  this  register  work?  How  does register  file  work  and  works  out  of  this? We're  going  to  learn  this  too. It  goes  all  the  way  down. The  way  we're  going  to  address this  is  like  a  biting  a  sandwich. Biting  like  a  ni  in  and  out  burger. Your  teeth  come  in  from  the top  all  the  way  down, top,  top,  top,  top,  top,  down  to  the  middle. Then  your  bottom  jaw activates  and  it's  going  to  go  bump  up  up. So  we're  going  to  start from  the  lowest  level  of hardware  all  the  way  up  into  the  middle. So  we're  going  to  meet  in  the  middle. Does  that makes  sense?  Top  down. Now  you're  at  the  ISA  now. Now  we  know  what  we  want  to  build. We  want  to  build  a  machine  that runs  those  ones  and  zeros. Then  we're  going  to  take  bottom  up, start  from  gait,  start  from electrons,  start  from  switches. Very  basic  work.  A  way  up, working  way  up,  working  way  up. And  all  of  a  sudden  now  we're  gonna  have a  machine  that  actually  runs  these  ones and  zeros  exhibit  mind blowing  when  you  finally understand  the  whole  thing, top  to  bottom  and  bottom  to  top  makes  sense. Awesome.  Alright.  Beautiful  who? It's  very  exciting.  I'm  sorry, I  said  it's  turtles  all  the  way  down. It's  like  abstraction  layers all  the  way  down. You  just  have  to  worry  about  your  layer above  you,  in  the  layer  below  you. It's  really  beautiful  thinking  about  that. All  right.  Grander  number  two  on  time. Look  at  this  time.  I'm  so  happy.  Moore's  Law. Moore's  Law.  Who's  more,  who's  this  more  guy? That's  Gordon  Moore.  Oh,  BS calm.  He  sat  right  here. This  building  was  built  before 1950.  He  sat  right  here. This  is  Gordon  Moore, Intel  co  founder,  just  passed  away. Amazing  guy,  okay,  predicted. He  saw  this  curve  of what's  this  curve  look  like? This  is  linear,  just  time. This  is  a  logarithmic  plot  of  the  number of  transistors  on  an  integrated  circuit. Transistors  on  an  integrated  circuit in  a  logarithmic  scale. Now  he  saw  a  straight  line from  a  couple  of  the  transistors. And  he  said,  this  is  very  interesting. I'm  not  saying  I'm making  a  law,  it's  not  like  he  was. So,  you  know  what  kind  of hubris  is  that  he  just  said? I  predict  that that  straight  line  will  continue  Now, does  anybody  know  what  this  means? If  this  is  a  linear  plot  in  time  and a  logarithmic  plot  in  the  y  axis? What  kind  of  shape  of  curves  is, you  might  know  that  word. It  starts  with  an  E123  exponential. Okay?  So  he  predicted this  curve  which  was  growing at  doubling  every  two  years. He  says,  I  predicted  it'll  continue. So  he  predicted  the  dots.  You  make  sense. What's  that  dots  mean? Transistors  on  an  integrated  circuit. Okay,  Transistors  on  the  chip.  You  get  it? Okay,  well,  what  happened? Well,  it basically  followed the  path  that  he  predicted. Roughly,  roughly  right. It's  still  a  straight  line. Roughly  a  straight  line. And  at  some  point  it  even  ticked  up to  be  doubling  every  18  months. This,  I'm  insane.  I'm  just in  awe  of  all  the engineers  who  made  this  happen. Many,  many  years  of  engineers have  made  this  happen,  contribute  to  this. And  in  fact,  they  used  to  use the  Moore's  Law  Curve  to  where  they would  aim  their  engineering  efforts  like, oh,  you  know,  we're  over  here  in  1985, let's  make  sure  that  we're  in  that  range. So  they  would  use  this  curve roughly  to  kind  of  keep  on  the  line. So  it  wasn't  like  it  was happenstance  that  this  is  a  straight  line. The  engineering  teams  use this  as  a  guide  post  for  them. That  makes  sense.  Pretty  cool. And  you  can  see  maybe you  know  somebody's  names. 486  pentium  itanium. Maybe  you've  heard  those  names  before. These  are  all  the  processors  and  the  number of  all  we're  looking  at,  transistors  per  dot. We're  not  looking  at  performance, just  transistors  per  dot. Pretty  impressive.  And  by  the  way, you'll  notice  memory  follows  the  same  path. It's  pretty  amazing. So  memory  has  also  been  exponential. Pretty  cool.  All  right. Here  are  some  pictures  of  some  chips. It's  pretty  cool. 8,008  the  pentium. Probably  You've  heard  about. Now,  other  things.  Here's  some  other  grad. Just  kind  of  a  pause  now  to relax  and  joke  for  a  second. This  is  the  curve  from Teachers  College  record  in 2012  of the  grade  distribution  over  time,  nationwide. And  you'll  see  the  A  curve is  doing  pretty  well. Look  at  this,  this  is  doing  very  well. And  so  the  great  news  is  that your  grandchildren  will  all get  a  so  we're  pretty  confident  about  that. We're  feeling  happy  about  that. But  actually  one  of  our  efforts  is called  as  for  all  we're  trying  to change  education  to  say  this  should  be  a, you  should  have  a  project where  you  can  keep  pinning the  auto  grader  infinite times  until  you  get  it  right. We're  building  under  61, so  we're  building  in  that  kind  of model  that  you  should, you'll  just  get  one  shot. You  should  get  multiple  shots. We  don't  yet  have  multiple  shots  in  exams. We're  working  on  that.  We  have  it  in  CS  ten. We're  trying  to  bring  into  61  C, but  we  have  this  idea  long goal  that  we  should have  as  for  all  but  not frees  the  same  as  always, the  same  as  from  every  semester. But  you  should  all  be able  to  get  an  A  in  the  future. That's  what  we're  believing. We're  all  excited about  that  initiative,  we're  working  on  that. Pretty  cool  stuff.  Okay, So  this  is  a  half  a  joke, but  half  actually  serious. We  actually  care  about  this. And  try  to  go  to  a  point  where  all  students can  succeed  Moore's  Law.  Well,  how  did  it  go? Well,  look  at  this.  This  is  actually a  beautiful  graph  of  Amaze. If  you  zoom  in  on  the  slides, hopefully  by  the  way, the  slides  should  be  on  line. If  they're  not,  let's  make  sure  our TA's,  the  slides  are  on  line. So  by  the  way,  I  love  people  following along  on  laptops  and  ipads. These  slides  are  gorgeous  color. If  you  can't  see  it  on  the  screen, you  go  to  pull  up  in your  ipad  and  put  up  on  your  laptop. So  please  pull  those  up  and I  hope  the  slides  are  online. I  had  them  ready  to  cup, you  know,  last  last  night. So  hopefully  they'll  move  on  line. This  is  delightful.  This  is  delightful. Now,  something  happened  around 2005  and  it's  kind  of  slowed  down. We're  going  to  talk  about  that, it's  a  whole  conversation. And  talk  about  that  when  that  happened. Okay.  Credit  in  number  three, principal  of  locality  and  memory  hierarchy. This  is  Jim  Gray,  also a  bachelor  degree  from  Cal, Also  a  Phd  from  Cal.  A  Turing  Award  winner. Who  knows  where  the  touring  award  is? Anybody?  Bueller? Bueller,  yeah. Hand.  Timmy,  tell  me  your  name. Luka  Go.  Nobel  Prize to  award  is  correct  by  the  way. Well  then  the  touring  award  is the  Nobel  Prize  in  Computer  Science, our  own  Callum,  whoo  Jim  Gray got  a  Nobel  Prize  Computer  science. Pretty  cool  stuff. One  of  the  things  he  did,  not for  this  work,  but  for  other  things  he  did. But  one  of  the  things  he  did  in  his  wonderful storied  career  is  that he  made  an  analogy of  how  long  it  takes  to  get  data, depending  on  where  you  store  it. And  the  analogy  is  if  I  have something  in  my  head  like  I've  written my  password  down  on  a  piece  of  paper, on  a  post  it,  I  don't  remember  what  it  is. I've  written  it  down  on  a  piece  of  paper. How  long  would  it  take  me  to  get  it  if  I had  to  go  back  to  my  room? Well  or  campus?  Maybe  10  min. What  if  I  left  a  piece of  paper  in  Sacramento, then  It's  like  an  hour  and  a  half. You  like  the  actual  drive  to get  it?  An  hour  and  a  half,  right? What  if  I  have  to  go  to  Pluto? Like  actual  Pluto, with  technology  we  have  today, it's  about  two  years  to  go  to  Pluto. What  if  I  have  to  go  to  the  drama  galaxy? That's  like  2000  years. See,  you  obviously  want  to keep  it  closer  to  you,  right? If  I  want  something,  I  want  to  use often  this  password  all  the  time. Mac,  remember  it? I  want  it  as  close  as  I  can. Makes  sense,  right?  You  don't  want  to  go to  Kluto  every  time  you  need  it. Well,  this  is  the  analogy  of  how  long  it takes  to  get  data  into  the  CPU. If  it  were  in  a  register, you  don't  know  what  that  is. It's  a  little  piece  of  storage we  use  that's  on  the  die, that's  like  in  your  head,  super  fast.  1  Ns. Well,  onboard  cache,  you  don't, that  is  it's  onboard  storage  pretty  fast. You'll  learn  later,  it's  about  10  Ns. Memory  is  about  100. Notice  the  equivalent  of  like  1  min to  1.5  min, right?  About  100  times  more. Okay?  Disc  is  1  million  times  more. What's  1  million  times  a  minute? Two  years  you  get  it. So  when  you  go  to  disc,  it's  like literally  going  for  two  years  to  get  data. It  makes  sense  in  that  analogy. And  if  I  have  to  ever  go  to  tape  or some  robot  doing  the  robot  here. And  it  goes  and  picks  up  the  thing  and puts  the  tape  in  the  thing, and  loads  the  tape  in. That's  like  1  trillion  times  1  trillion  Ns, or  like  going  to  drama. You'd  never  want  to  do  that.  You  might need  to  something  once  in  a  while, but  you  never  want  to  do  that  in  general. So  this  is  a  kind  of  a  relative to  give  you  a  sense  of  why  you want  to  keep  data  as  close  as  you can  to  the  processor,  as  low  as  you  can. Don't  put  it  far  away  in  disc  or  on  tape. Makes  sense.  Okay,  beautiful  analogy. This  is,  we're  going  to  spend a  lot  of  time  talking  about  this. This  is  that  same  analogy,  but  at  the  top, here's  my  CPU  core  needing  data. There's  a  register  really  fast  in  my  head. There's  my  caches,  there's  memory, here's  virtual  memory, we're  going  to  learn  this,  we're  going, here's  solid  state  memory,  and  here  is  disc. So  that  same  picture  is  now flipped  where  the  thing at  the  top,  by  the  way, what's  really  cool  about  this  picture, every  storage  layer  has a  copy  of  the  layer  below  it. So  that's  actually  pretty cool  because  it's  smaller. So  it  can  hold  a  copy  of  the  layer  below  it. Not  all  of  it.  I'll  say  it  again. Every  layer  below  has a  copy  of  the  smaller  one  above. I'll  say  that  correctly.  Okay.  This  is just  some  subset  that's  in  my  smaller  copy, but  I  can't  hold  all  of  it  below. The  one  below  holds  all  of  the  guys  above. Okay.  That's  what  I'm  trying  to, I  meant  to  say. Everybody  below  has  three  properties. Everybody  below  has  three  properties. One,  it's  bigger,  that's  the  size. I'm  showing  you.  It's  slower. We  talked  about  that  a  second  ago. And  it's  cheaper  per  bit. Three  things.  Bigger,  slower, and  cheaper,  okay? Reversing  that  the  small  guys. Smaller,  faster  but  crazy  expensive  per  bit. Okay?  I'll  the  same  idea, number  four.  Great  idea  number  four. We're  doing  amazing  with  great  number four,  parallelism. We're  going  to  see  parallelism in  so  many  different  dimensions. You  see  all  the  metal  boxes, parallels  and  parallels  and parallels  and  parallels  and  peris. This  first  is  called instruction  level  parallelism, in  which  can  you  imagine it  at  a  certain  slice  in  time? This  instruction  is  finishing this  instruction  two  is writing  to  memory  or  reading  to  memory. This  instruction  three  is  doing some  execute,  let's  doing  edition. This  instruction  four  is  doing  a  decode. What  is  it?  What's  happening? What's  the  control  path  for  it? And  this  instruction  five  is  getting  it for  a  memory  all  at  the  same  time. I  can  have  five  instructions at  different  stages. Think  of  an  assembly  line  member. Assembly,  Assembly  line,  right? Assemble.  Here's  a  assemble  going  through. At  any  one  stage,  this  car is  being  painted,  almost  finished. This  car  is  just  getting  painted. It  make  sense?  This  car  is  having a  door  put  on  this  car. The  framework  is  going,  this  car is  just  rolling  into  the  lot. Makes  sense.  On  the  same  time, that's  the  same  idea. Five  instructions  at different  stages  of  their  completion  cycle. Pretty  powerful,  pretty  cool  stuff. Parallels  number  one  called instruction  level  parallelism. Pals  number  two,  I  said  this  story. Parallelism  number two  is  thread  level  parallel. You  probably  know  about  this. Oh,  you've  heard the  word  threads. You  might  not  know  when  they  were. Oh,  I  want  more  threads. More  thread  thread. We're  gonna  teach  you  what  those  are. This  is  thread  level  parallelism  where  you, the  controller,  the  C  programmer  can  say, I  would  like  to  have  20  threads  and  all  of a  sudden  20  different  copies of  the  code  start  running  at  once. And  they  might  come  together  at the  end  and  join. So  that's,  that's  called  thread level  or  TLP  or  thread  level  parallelism. So  far  so  good.  Well,  if  feel free  to  ask  any  questions, rays  hand,  this  is  really  fun. Hopefully  we'll  have  this  be  interactive once  we  get  past  lecture  one, lecture  two,  lecture  three. This  is  just  really  fast.  It's  kind  of  like a  whoo  whirlwind  drinking  from  a  fire  host. The  third  kind,  it's called  data  level  parallelism, in  which  you  might  have  a  Google  Doc with  three  different  people writing  different  parts  of  that  report. And  that  Jane  is  going  to  do something  and  Sue's  doing  something and  Tom's  doing  something  all in  at  a  higher  level. Really  high  level.  Not  hardware  level, but  just  high  level. You  can  somehow  use  different  resources  and do  paralleled  three  different  people's  work. It's  really  awesome  data  level, DLP,  data  level  parallelism. Now  is  all  this  amazing, Like  if  I  had  ten  processors that  I  always  ten  times  faster. If  I  have  ten  people, I'm  always  ten  times  faster. Do  you  ever  like  have  a  project? Are  you  always  exactly if  you  have  two  people, you're  always  doubly  fast? No. You're  usually  a  little less  than  twice  as  fast. Do  you  ever  think  about  that?  Alright? If  you  have  ten  people  making  bread, are  you  ten  times  faster  making  that  bread? No.  Why?  Well,  shared  resource is  only  one  oven  and  other  things  like  that, but  also  something  called  Amd  law  which  says the  speed  up  you  get  is  a  function of  how  serial  the  process  is. If  you  have  some  processes  where  like only  one  think  can  go through  is  only  one  oven. Or  before  you've  decided  to divide  the  work  to  think  about  something, think  about  something  and then  divide  the  work. That  cereal  part  is  going to  control  how  parallel, how  fast  your  speed  up  can  be. Okay,  That's  all  it  is. Okay.  Pretty  cool  stuff. So  poop,  performance  game,  I'll  read  this. Performance  games  are  limited  by  the  ratio  of software  processing  that  must be  executed  sequentially. It's  all  about  that  cereal  part. That's  really  the  bottleneck. Okay.  We'll  get  to  that.  We'll  talk about  why,  what  to  do  with  it. But  it's,  it's  called Om  Dls,  Heartbreaking  Law. There's  no  way  around  it. It  is  literally  a  law. It  is  heart  breaking  that  you  can't get  faster  than  that.  It's  pretty  crazy. Okay.  Number  five. Almost  done,  great  idea. Number  five,  Performance measurement  and  improvement. How  do  you  know  if  you're  faster, if  you  never  measure  anything? That's  what  the  point  of, the  main  point  of  this  is. So  you  match  the  application  to the  underlying  hardware  to exploit  some  cool  things. Locality,  Locality means  code,  the  data  is  closer  to  you. Parallelism,  we  talked  about that  special  hardware  features that  you  might  have. Trying  to  think  about  22  parts of  performance measuring  one  is  called  latency. Latency,  how  long  something  takes, Just  time  measure, you  watch,  ready,  poop,  boop. That's  latency,  okay?  Or  throughput, which  is  how  much  work  can you  get  done  overall? That's  throughput.  So  you  want to  optimize  one  or  the  other  or  both. But  thinking  about  those  two  measures of  performance,  la, latency  and  throughput, this  is  great  idea  number  six. Dependability  versus  via  reliability. Via  redundancy. Dependability  via  redundancy. Who's  ever  been  to  the  exploratorium? Raise  your  hand.  Exploratorium  is the  greatest  science  museum  in  the  world. It  happens  to  be  in  San  Francisco. Peer  who's  who  give  me  a  number  per  number, 14,  15,  something  like that.  Somebody  look  it  up. Go  to  this,  take  a  weekend. If  you're  free  and  go  to the  exploratorium  with  their  friends, you  will  be  exhausted  but  learn  so  much. The  best  science  museum in  the  world  is  in  our  back  yard. Folks,  this  is  called  a  cloud  chamber. Let's  let  you  see  cosmic  rays. Visualized  cosmic  rays  hitting us  literally  all  the  time. We  put  a  lead  shield,  it  doesn't  matter. Put  the  Earth  between.  No,  no, half  of  these  things  come from  underneath  the  Earth. Do  you  think? There's  not  a  lot  of  lead  and  a  lot of  mass  be  below  the  Earth? Below  the  sensor?  No  stuff, many  of  these  clouds  are  coming from  below  the  Earth  through. Nothing  stops  them.  It's  unbelievable. And  they're  visualized  in  this amazingly  called  a  cloud  chamber. Go  see  this. It's  one  of their  first  exhibits  they've  ever  had. And  it's  old.  You  can  tell.  It's  the  old  kind of  1956,  but  it's  so  good. It's  like,  oh  my  god,  look, it's  hitting  me  all  the  time. What  happens  when  those  cosmic  rays happen  to  hit  a  very  small, getting  smaller  and  smaller every  day  hits  a  transistor  poop. All  of  a  sudden  that  transistor  goes 0-1  You  get  that? That's  bad.  You  can't stop  it,  you  got  to  live  with  it. So  you  have  to  have  redundancy look  dependability. Via  redundancy,  you  have to  be  able  to  support  the  fact that  some  of  these  ones  are gonna  flip  randomly. And  you  hope  not  all  of  them  flip  at  once. Maybe  just  one  here  and  one  there. And  maybe  you  can  deal  with  that. That's  powerful.  This  is  such  a  cool  video. So  I  want  to  thank  you the  exploratory  for  doing  this. So  how  do  we  do  that?  Oh  yeah,  Question. Go  measure  the  probability  and  is there  like  a  accepted  standard? That's  a  really  good  question. How  do  you  measure  like  how  do  you  know  this? Isn't  there  some  number  or probability  of  thinking  about  that? They  certainly  look  at  what  kind of,  in  general,  for  any  system, you  look  at  what  kind of  errors  you're  going  to  get  and try  to  handle  the  kind  of errors  that  you  typically  get. And  that  might  be  if  you're  a  driver  of, if  I  rent  a  fleet  of  trucks, what's  the  kind  of  errors  I  get? Well,  flat  tires  and  other  things,  right? So,  and  we  just had  a  hurricane  hit  Southern  California, but  hopefully  all  families  are okay  and  stuff,  but  we  had  a  hurricane. So  you  think  about  the  kind  of  errors and  how  to  deal  with  that  probabilistically. So  in  every  single  case,  it's  different  as you  look  at  what  kind  of errors  you  typically  get. That's  a  great  question.  So  how do  you  do  this  redundancy? I've  got  three  boxes, each  of  them  computing addition  of  two  numbers. And  one  of  them  says  one  plus  one  is 21  plus  one  is  21  plus  one  is  one. Something  happened  to  that  box. That  box  could  die,  right? You  might  have  a  Internet. We  just  had  the Internet  wire  didn't  work  this  morning. So  I  have  Wi  Fi  as  a  backup, dependability,  a  redundancy. I  had  Wi  Fi  and  I  had  a  wire. We've  got  two  instructors.  I  get  sick. Justin's  here.  Justin  get sick.  I'm  here.  We're  all  together. We've  got  100  A's. Right. All  of  that's  dependability,  redundancy. So  in  this  case,  if  two  out  of  three  agree, I'll  just  go  with  the  majority. Makes  sense. That's  the  way  you  have  to  be  able  to  handle the  fact  that  one  of  them  might  have a  bit  that  flips  and  that's  okay. Pretty  cool.  Now,  increased transistor  density, which  you  saw  those  numbers going  up  and  up  and  up  means  that the  cost  of  extra  transistors to  make  that  redundancy is  not  a  big  deal  anymore. It  used  to  be  a  big  deal  when each  bit  was  expensive.  Now  it's  super  cheap. I  mean,  relatively  so,  so  I  could  just  throw another  transistor  to  have some  redundancy  here  to  do  that. Pretty  cool. So  what  are  the  redundant  things  that  you might  see  in  your  life  that you  probably  know  about? Did  you  know  that  they have  redundant  data  centers that  literally  Amazon and  Facebook  and  the  people  who  have massive  piece  of  data  that they  can't  afford  to  lose. All  of  a  sudden somebody  accidentally  went  bloop and  unplugged  the  eastern seaboard,  which  has  happened. They've  had  storms  take out  the  power  and  the  whole  Eastern  Seaboard. Guess  what  happens?  Amazon  doesn't go  down.  How  is  that  possible? 'cause  they  have  that  entire  data  center worth  duplicated  somewhere  else  far  away. Not  like  neighbor  far  away. So  if  this  one  goes  down,  they've  got  it  up. Is  that  amazing?  Crazy  stuff. So,  and  a  whole  data  center  is redundant.  Isn't  that  unbelievable? That's  pretty  cool  stuff. Redundant.  Who's  ever  heard  of  raid redundant  array  of  inexpensive  disks? You  know  where  we  invented  that? Who  invented  that? You  see  Berkeley.  My  colleague Randy  Katz  and  his  team  invented  that. Berkeley,  Berkeley's  name  is  all over  this  course  and all  over  computer  architecture. It's  amazing  stuff.  Memory,  bits. Who's  ever  heard  of  ECC or  era  correcting  code  memory? That's  pretty  cool.  Look  at  this. How  many  boxes? 1,  2,  3,  4,  5,  6, 7,  4,  5,  6, 7,  8,  9,  9. Wait,  but  there's  eight  bits  in  a  byte. Yeah,  Redundancy,  isn't  that  cool? Kind  of  cool  stuff. All  right,  so  data  centers, discs,  memory  bits,  pretty  cool  stuff. Dramatic  pause  it,  give  me  a  little, give  me  a  little  like,  give  me a  little  dramatic  pause,  pause,  pause,  pause. Isn't  this  old  stuff,  isn't  it? Like  math,  a  calculus  hasn't changed  for  hundreds  of  years. Okay,  realistically. So,  their  classes  are  like, oh,  calculus,  it's  the  same. Calculus.  Don't,  don't  get  excited. It's  the  same  calculus,  but computer  architecture is  crazy  exciting  today. Why,  why  does  this  happen? What's  so  exciting  about  it? Well,  there  were  different  waves  of  what computers  were  used  for  in  the  early  days. We  used,  you  know,  they  were  super  expensive. They  were  only  a  universities  and  companies. They  were  main  frames  for  data  processing. Ibm  was  using  it  for  payroll, and  I  think  they  tabulated  some  census  or something  on  there  and  I mean  they  did  big  things,  Okay. But  no  human,  like  no  user, no  average  Joe  was  having  it. Then  you  had  the  '80s  and  '90s,  Steve  Jobs, A  personal  computer,  right? Microsoft,  Pretty  exciting. Ibm  and  Apple, putting  out  heart,  professional  computers. The  web  was  very  exciting. Then  you  had  smartphone  apps. That  was  pretty  exciting  for  a  while. Now  look  at  all  those  applications where  computers  really  used for  serious  compute  healthcare, telemedicine  education. All  the  mooks,  autonomy, gaming,  entertainment, VAR,  and  visualization. To  list  ten,  to  list  ten. There's  so  many  applications where  you  need  fast  compute. It's  such  an  exciting  field, really,  really  cool. There  is  really  no  killer  application. If  you  ask  an  architect like  what's  the  killer  app? There's  like  machine  learning  maybe. But  really  it's  all  over. We  need,  we  need  everybody. We  need  all  hands  on  deck. Okay?  It's  pretty,  machine  learning is  common  for  most  domains. This  is  the  curve  I  might  have  shown  before. I  love  this  picture.  We're  going  to  get  dig, dig  deeper  into  this  picture. This,  what's  really  beautiful is  it's  a  unit  list, y  axis,  because  the  units are  on  each  curve.  Isn't  that  clever? It's  really  clever.  The  number  of transistors  on an  integrated  circuit, what's  that  curve  called? 123  more. That's  the  Moore's  Law  Curve. I  just  said  that  before. Remember  that  one?  Moore's  Law,  Gordon  Moore. We  can  also  look  at  the  frequency and  how  that's  tapered  off. That's  interesting,  but  also the  power  is  going  up  so  high. We  want  that  to  taper.  We  don't  want  a  lot of  power  green  emissions. You  want  things  greener, so  you  want  them  flatter. So  you  want  to  be  lower  than  that. But  look,  this  is  ex  crevice  is  shown  here, This  is  right  here, the  sequential  app  performance. So  if  you  only  run  a  program that  has  no  parallelism, it's  going  to  also  taper  off because  computer  speed  isn't really  getting  that  much  faster,  roughly. You  get  more  transistors,  yeah. But  it's  not  getting  any faster  in  terms  of  the  megahertz, the  heartbeat  of  the  system. The  power  is  stable.  That's  good. But  the  sequential  app  performance  is  stable. That's  not  good.  We  have a  hunger  need  for  usage, so  what  has  continued? Well,  they  started  increasing the  number  of  cores  on  a  computer, the  number  of  tiny  micro  cores within  CPU  because  of  that. Now  the  parallel  performance keeps  growing,  so  that's  a  good  thing. Okay,  we'll  get  into  this  more  details, but  that's  that  reason  two, there's  something  called  DSA, or  domain  specific  architectures here  called  domain  specific  computers,  DC's. Each  domain  I  talked  about  health  care,  VR, AR  needs  a  heterogeneous  system of  multiple  cores,  Maybe  some  GPU's, graphics  processing  units, maybe  some  neuroprocessurits, accelerators  interfaces,  memory,  et  cetera. Here's  an  example  of  Apple  15  bionic  chip. So  the  old  conventional  wisdom  was, and  this  is  a  slide I'm  doing  pretty  well  for  time. Moore's  Law  plus  denard  scaling. Nard  scaling  says  you don't  have  to  do  anything. Just  the  fact  that  the  clock  rate  is  going up  means  you're  going  to  get free  performance  increases. Just  sit  back  and  be  called the  lazy  programmer  syndrome. You  don't  have  to  optimize  your  code, just  let  it  go. It'll  just  be  faster  next  year because  the  computers  are  getting  faster. Okay,  That's  what  it  is. Faster.  Cheaper,  lower  power, general  purpose  computers. Every  year.  Like  that  was  it. That  was  the  idea. In  the  glory  days,  1%  a  week, like  literally  you  woke  up  the  next  weekend was  1%  faster  overall. That,  that's  a  pretty  good  return  rate, you  guys  to  put  money  in  a  bank,  that's a  pretty  good  return  rate,  1%  a  week. Now,  it  was  dumb  to compete  by  designing parallel  with  specialized  computers. Why? Because  just  the overall  general  purpose  computers would  eventually  get  faster  than  that. So  why  spend  two  years  in a  research  program  making a  special  purpose  thing, when  the  average  program  you  buy  from  Dell or  Apple  is  going  to  be  faster than  the  thing  you  custom  designed. So  people  didn't  do  that. See,  that's  what  it  says. Okay.  But  now  they've  all  flattened. Yes,  You  can  go  more  parallel, but  they're  not  getting  fundamentally  faster. You  don't  have  denard  scaling  anymore. The  end  of  denard  scaling  is  here. You  just  can't  wait,  and wait  for  the  CPU  speed, clock  speed  to  all  give  you that  performance. You  don't  have  that  anymore. So  it's  time  to  start developing  your  own  the  hardware. All  of  a  sudden  it's  like,  oh  my  gosh, we  can  finally  you finally  let  the  dogs  loose. You  have  dogs  who  want  to  run  at  a  park, you  take  off  the  chain,  They  like  go  crazy. They  all  like  smelling  each  other. You've  done  that. They're  all  smelling  and  they're doing.  We  have  that. That's  our  architecture. That's  all  the  architects are  finally  let  go  from  their  leash. Go  run,  be  wild,  be  free. And  they're  building  unbelievable  hardware. Look  at  this,  the  Google  TPU, three  specialized engine  for  training  neural  networks. Unbelievable  performance, unbelievable  performance. That's  a  big  number.  Petaflops.  Flo  stands for  Floating  Point  Operations. This  thing  can  do  a  ton  of compute,  unbelievable,  unbelievable,  fast. And  by  the  way,  we  talked  about conventional  wisdom  and  kind of  flouting  conventional  wisdom. And  may  I  know  who  these  two  people  are? David  Patterson,  my  colleague who  taught  me  61. He  sat  with  me  together  in Wheeler  20  years  ago and  taught  me  how  to  teach  61. So  I  really  owe  a  lot  of  what  I've done  to  David  Patterson  and  also the  transformations  to  take  this  course into  paralism  was  Dave  Patterson's  idea. So  great  ideas  have  come  from  this  man. Amazing  leader,  amazing educator,  good  friend. And  this  is  John  Hennessey, who  was  the  President  of  Stanford, also  an  architect,  He  worked  his way  up  the  administration.  Stanford  side. Patterson  was  always  just  a  professor  here, but  full  professor,  amazing  works  to  Google. Now  anyway,  the  point  is  they  got the  churning  award  and  well  deserved  for this  idea  to  flout the  conventional  wisdom  that a  kind  of  assembly  language  should, should  be  super  minimal. A  reduced  instruction  set computer  is  what  they  argued. They  got  literally,  I'm  not  joking. They  got  laughed.  They  used  to  have panels  and  they'd  be  laughed  at. Like  imagine  you  go  to a  conference  and  you're  here  with  your  tie, and  you're  laughed  at  by  your  colleagues. How  good  did  it  feel 20  years  later  for  them  to  say, you  write  all  along  and they  give  them  the  churning  award. Isn't  that  amazing? That's  powerful  stuff.  So  they defied  conventional  wisdom. These  two  leaders  are amazing  and  they  used  to  write  the  book, in  fact,  your  book  is  Patterson  and  Hennessy. It's  their  book.  Pretty  impressive,  right? By  the  way,  that's  the  most  popular  book  in all  the  world  in  terms  of  architecture. No  book  is  number  one. That's  number  one,  and  way farther  ahead  of  number  one. Number  two,  it's  like  Steph  Curry, farther  ahead  than like  anybody  else  in  books. That's  Patterson  Hennessy. All  right,  here  we  go. 15  min,  done  with  the  big  six  ideas. Now  let's  talk  about this  class  and  the  details and  all  the  kind of  red  tape  they  need  to  know. Here's,  here's  my  yoda  imitation. We've  made  a  lot  of  planning,  just and  I  have  taught  this  course  before. We've  got  amazing  head  TA  group, a  major  group  of  TA's, many  of  them  are  veterans,  a  lot  of  veterans. So  as  much  as  we  planned  things  will  change. We're  going  to  have  smoke  issues  in  this. We  might  move  something around  or  the  project  took  too  long, etcetera,  et  cetera,  et  cetera. So  here's  my  yoda  imitation, always  in  motion  the  future is  that  was  my  attempt. All  right.  The  point  is we  tried  as  much  as  we  can  to  lock  it  in. To  lock,  obviously  be flexible  with  us  as  we  might  need  to change  your  schedule  in the  future.  All  right.  Very  fast. Website.  Is  there  61 C.org  Here's  our  course.  E  mail. Do  not  e  mail  or  DM  Individual  TAs or  instructors  with  course  ltd  questions only  go  through  this  path.  It  makes  sense. Say  yes.  Yeah,  I  ask. Okay.  Dan,  adjusted  instructors,  TA's, we've  got  a  huge  web  page, Ed  stem  is  our  common  form. You  probably  used  that  before. Optional  readings.  These  are the  Patterson  Hennessy  book  I  talked  about. K  and  R.  Regan  and Ritchie  is  the  C  programming  book. And  this  when  we  get  to  the  data  center. This  is  a  little  bit  of article  that  you  read  about the  data  center.  Okay.  Pretty  easy. Okay.  As  required  reading, read  our  course  policies  page  because  we  have a  couple  of  unique  policies  that  are different,  6-18  and  B. Make  sure  you  read  about  those. To  see  though,  here's some  summary  of  the  elements. Number  one,  lecture,  Monday, Wednesday,  Friday  for  an  hour. In  person  or  in  zoom. We,  we  obviously  welcome everybody  on  zoom.  We're  zooming  right  now. Hi  everybody  to  zoom,  wave,  zoom,  zoom  way. Okay,  here's  the  key. By  the  way,  you  can  be  on  zoom  in  class. This  just,  you're  eating  up the  Wi  fi,  but  that's  fine. We'll  work  with  that.  We  have  900. Are  we  looking  at  900? How  many  students  we  have  this  semester? How  many  students  we  have? This  can  be  a  sense,  roughly  100 within  100.  Give  me  a  number. Okay,  sorry,  750. This  room  doesn't  hold  750,  unfortunately. So  we  need  some  of  you  to  say, you  know  what,  I'll  sleep  in, I'll  watch  it  on  zoom. I'll  be  there  because otherwise  we  can't  have  it  there. We  have  one  room  on  campus  that  hold  750 and  that's  wheeler  and  it  wasn't available.  So  we  do  what  we  can. Okay.  So  some  of  you  will  and it's  also  hot  and  this  and  Covid. So  some  of  you  watch  online. And  if  you  almost  all  our  resources  online, if  you  get  Covid  or  you  get  sick, but  hope  no  you  do, please  wear  a  mask because  it's  coming  around. Now,  you  guys  know  what's  coming  around. The  point  is,  if  you  happen  to  watch  online, we  have  almost  all  the  resources  online. You  can  keep  following along  from  your  bed,  okay? So  don't  worry  about  that. Okay,  discussion,  homework, design,  analyze,  illustrate,  have some  fun  with  that  lab  and  projects. Implement,  simulate,  debug, midterm  and  final  rock  on show  your  stuff  kind  of  standard  in  that  way. Lab  zero  is  set  up, so  you  have  your  Github set  up  and  everything  set  up  there. It's  all  just  set  up  this  week  and  lab one  and  discussions  start  next  week. No  discussions  this  week, no  sections  and  no  labs  this  week. Lab  zero  is  set  up. Okay.  All  right,  here's  more  course  elements. We  have  office  hours  in  person  or  hybrid. They're  based  instructors, our  instructor  office  hours are  going  to  be  conceptual. Don't  say  Dan,  Help  me  fix  my  project  six, I  can't  help  you.  I'm  not  going  to  help  you. My  job  is  to  help  you  rock  on  the  exam. Come  say,  Dana,  help  teach me  this  material  you're learning  that's  gonna  be  test  on  exam. That's  worth  like  so  many  points. That's  what  I  help  you  with. That's  called  conceptual  office  hours. Okay. Come  to  TA  office  hours  for  your  project. Makes  sense. Okay.  Full  schedule  construction. We'll  get  that  up  soon. We're  going  to  have  project  parties, a  chance  for  you  to  have  extra office  hours  around  project  time. But  no,  none  officially,  but  we're  going  to have  extra  office  hours  to  support  you. Come  project  time,  you'll  be  fine. Here's  the  most  important  thing. We're  going  to  have deadlines  for  all  the  projects. If  you  cannot  meet  a  deadline, all  you  got  to  do  is  reach  out  to  us. We  have  an  extension  form  and  a  really, it's  actually  being  adopted  by almost  all  the  courses  around  EECS  this  year. We're  really  happy  about  that, We  will  work  with  you. If  you  say,  Dan,  I  need  an  extra  week, we  will  give  you  the,  we  just  talk  to  us. Okay. So  this  extension  form  is  a  week  to  say  like I  want  to  meet  your deadline  just  a  little  bit  more  time. Because  something  out  of  my  life, no  problem.  We'll  always  say  yes. But  let's  just  talk  to  first. Okay?  Extension  form. Okay.  Enrollment.  Right  now, it  looks  like  we're  kind  of  full. We've  got  20  on  the  wait  list. Don't,  don't  email  us  about  the  wait  list. Again, that's  not  something  we're going  to  deal  with. We  are  trying  to  expand,  we'll let  you  know  in  lecture  if  that  happens, concurrent  Roman  students who's  a  concurrent  Roman  story, give  me  a  hand  up  for the  concurrent  Roman  students. Thank  you  all.  You're  awesome  Roman  students. Yeah,  you  know  why. You  know  why. For  every  ten  concurrent  Roman  students, we  get  20  more  non  concurrent, 20  more  regular  Berkeley  students. So  we  want  more  of  them  to invite  your  friends  to  take  it. So  we  can  expand  the  class so  that  we  have  everybody  in  the  class. We  want  to  support  everybody. We're  awesome.  They're  awesome. Thank  you  all  for  that,  for  doing  that. If  you  don't  turn  anything  by  week  three, like  I'm  not  doing  anything,  I'm  just gonna  just  bail  out. Then  we're  gonna  assume  you accidentally  sign  up  for  this course  and  didn't  mean  to  be  here. We'll  drop  you.  So  you got  to  do  something  by  week  three. Okay.  That's  it.  Okay.  I  I mentioned  this  extension  request  form. Let's  see  all  assignments  to  do  at  midnight. Let's  see,  Extension  form  is available  on  the  website, on  the  extensions  tab. Please  don't  hesitate  to  do. There's  no  shame  in saying  I  need  an  extension. Please  do  if  you  need  them. Really,  we're  here  for  you. If  stuff's  going  on  with  your  life, I  need  more  time. No,  again,  we're  not  going  to  penalize  you. Just  say,  I  need an  extension  and  you  will  get  it. Just  talk  to  us  first,  okay? Just  tell  us  what  you need  and  how  much  you  need. That's  all  it  is. Very  easy,  okay?  So  don't  stress, we're  not  gonna  cause  stress  in  this  course. We're  pretty  cool. And  by  the  way,  once  the  deadline  hits, we  may  have  less  office hours  support  'cause  like,  okay, here's  the  big  thing,  okay,  The  big  push for  the  delivery  where  it's  due. And  then  we  have less  office  hours  support  for  the  project. We'll  stop  a  little  bit,  but  not  a  lot. Okay.  So  just  let  you  know  that  we'll lower  our  office  hour  support. Once  the  deadline  hits,  I'm  going  to turn  over  to  Justin who's  going  to  talk  about  the  exam. Justin  is  the  exam  czar. I  know  of  nobody  I've  ever  taught  with. I've  taught  for  24  years  here, 34  years  of  you.  My  time  as  a  grad  student. I  know  of  nobody  I've  ever met  who's  as  good  as  thinking  about, really  thinking  about  what's the  right  question  to  ask  at  the  right  level, at  the  right  difficulty  than the  next  man  you  hear  from  Justin. Justin  gentlemen.  Hi. Hello?  Hello?  Yeah.  Al  right. This  is  working.  All  right. So,  yeah,  I'll  be  working  with  the  exams. We'll  have  a  mid  term  exam,  2  H  long. We'll  have  one  alternate  slot shortly  after  the  exam. If  you  have  a  conflict,  we're  not going  to  do  remote  exams  this  semester. We've  been  doing  it  for  covid  semesters, but  I  think  we're  trying  to get  back  into  in  person. If  you  really  need  a  remote  exam, let  us  know  and  we may  be  able  to  squeeze  you  in. It'll  probably  be  around  the  eighth  week. We're  still  figuring  out  rooms. We  have  a  final  exam, it'll  be  in  person  sometime during  the  final  exam  week. Standard  paper  exam  3  H.  We're generally  going  to  try to  have  about  two  thirds of  that  being  post  midterm  material, one  third  being  communal  questions. Same  thing  with the  midterm  one  alternate  slot. No  remote  exams.  This  semester we're  going  to  be  trying  something slightly  differently  with  our  grading. We're  going  to  be  guaranteeing  a 65%  minimum  average  on  all  of  our  exams. Let's  get  a  compuse  for  that. All  right,  beautiful  the  idea. The  exams  are  generally  the  largest  source  of variance  in  every  semester  grades. If  we  have  a  60%  exam  average, we're  going  to  assume  that the  exam  was  too  hard. We'll  bump  up  the  grades  to  make  it 65%  The  way  that  course  is  structured, labs,  and  homeworks,  and  projects, they  overall  cover about  60%  of  your  total  grade. The  midterm  and  final  is  going  to  be about  40%  We  do  this  because  we  feel  that projects  are  a  better  assessment of  how  well  you're  doing  in  the  class because  you're  not  in  real  life going  to  be  stuck  in  a  room doing  work  for  2  H.  If  you don't  get  the  answers  without  your  computer, then  you  lose  all  your  points. We  will  be  having  an  absolute  grading  scale. In  the  past  semesters,  we've  had  this  thing where  if  the  bins  are  too low  then  we  shift  them  up. We  are  not  doing  that  this  semester, our  bins  are  finalized. We're  using  the  65%  minimum  average to  guarantee  that  our  grading  system  works. Our  GPA  needs  to  between 2.83  0.3  due  to  departmental  requirements. So  because  of  that,  the  Bins  target target  a  3.3  GPA, assuming  65%  on  M  exams and  95%  on  every  other  assignment, 65%  is  a  minimum. If  everyone  does  really  well  on  the  exams, then  you  can  do  better  than  this and  you  can  increase  your  GPA. Everyone  can  get  an,  everybody says  that  you  can  all  get an  we're  not  going  to  curve  you. So  theoretically,  if  everyone does  really  well  on  the  exams, then  you  can  all  get  as. But  that's  awesome. We  are  still  targeting 65%  minimum  65%  averages. Good  luck,  I  guess. So  ultimately, don't  worry  too  much  about the  grade  you  get  here. What  matters  more  is  what  you  learn from  the  class,  right? When  you  get  a  job  internship or  a  job  interview,  right? The  grade  you  get  in the  class  is  just  the  foot  in  the  door. It  doesn't  really  matter.  If  you  can't  really explain  how  you're  doing  something, don't  focus  too  much  on  the  grade. Focus  more  on  learning  what  you  can. Our  goal  is  instructors, is  just  make  this  experience as  enjoyable  as  possible. So  we'll  have  some  fun,  some technology  in  the  new  things, we'll  have  some  challenging  projects, some  homework  questions  that really  get  you  to  think  hard. And  we  do  have  our  extension policy  as  generous  as  we can  just  to  ensure that  anyone  can  take their  course  on  their  own  pace. As  long  as  we  feel  that  you  can  actually succeed  in  the  course  at  that  pace. Right.  If  you  decide  to  wait and  do  all  of your  assignments  at  the  last  week, we  don't  think  that's  possible. So  we're  going  to  try  to push  back  a  bit  on  that. We're  going  to  keep our  cal,  standards  of  excellence, so  all  of  our  projects and  exams  are  going  to  be  just as  rigorous  as  every  other  semester. And  other  than  that, our  goal  is  to  just  make this  course  as  great  as  possible. We  do  need  to  discuss  a  bit about  how  our  assignments  work. Generally  speaking,  most  of these  projects  and  labs  will  be doing  either  solo  or  in  pairs. But  homeworks  need  to  be  done  alone. If  you're  working  in  a  partner group  for  a  project  or  a  lab, then  you  can't  work  with  other  partners, you  can  help  each  other  debug. We  don't  want  you  to  share  work  directly. The  general  rule  of  thumb  is  if some  other  group  could  have theoretically  learned  the  material, could  have  theoretically  gotten a  good  score  without actually  learning  material, then  we  would  consider  that to  be  over  collaboration. Our  goal  is  to  maximize the  amount  of  learning  that  gets  done. So  if  helping  out  helps other  people  learn  that  if helping  other  people  ends  up  harming their  learning  ability,  then  that's  not  good. Yeah.  Any  questions  here? Yeah.  What's  your  policy the  use  of  AI  for  discussion.  Good  question. What's  our  policy  on  the  use  of AI  for  discussions  in  code? I  would  personally  say that  you  can  use  it  with accreditation  the  same  way  as you  would  use  something  like  Stackoverflow, right? Stack  overflow  is  also  this  kind of  thing  where  if  you just  type  in  the  right  word, you'll  probably  get  the  right  solution  out. Chachi,  PT  and  AI. It's  probably  around  the  same  thing, slightly  better,  but  it's  about  the same  in  terms  of  what  you  can  use  with  it. Don't  try  to  just  look  up the  answer  to  this  project and  submit  that  as  your  own. I'm  fairly  certain Google  can  do  that  for  you, but  you're  not  learning  anything. And  like  I  said, the  grade  is  not  the  important  part. What's  important  is  what you've  learned  from  the  class. Yeah.  If  you  feel  like  you're  not  learning, then  don't  do  it,  All  right? Yeah.  We  did  have  a  lot  of cheating  cases  last  semester,  U. Just  as  a  general  rule,  you  can't  copy. So  you  can't  start  your  solutions. If  you  find  like a  partial  solution  on  the  web, don't  leave  your  code  anywhere out,  lying  around. We  did  have  some  incidents  where a  student  left  their  computer out  and  someone  copied  their  code. We  had  to  talk  with them  and  it  was  not a  fun  experience  for  anyone  involved. Yeah,  but  know  that  you're  responsible. If  you  do  it,  you're  responsible. Just  get  away  for  free. Oh,  I  walked  away,  they  copied  it. No,  you  also  are  responsible. That's  important. Our  general  policy  on  cheating  is  if you  catch  you  cheating  on  assignment, we  will  give  you  a  negative 100%  on  the  assignment. The  idea  being  that  we  want  to  ensure  that  no matter  how  unlikely  you think  it  is  that  you'll  get  caught, there's  always  a  negative expected  value  on  cheating. On  average,  you  will lose  points  if  you  cheat. Yeah,  we  have  caught  a  lot  of  students, and  if  you  give  code  willingly, then  we'll  be  treating  you  just  the same  way  as  if  you  received  code from  someone  else  after  the  semester  public. You  are  not  allowed  to  make your  code  public  after  the  semester  ends. That's  just  because  we  have  had  incidents where  students  use  some previous  students  code. Yeah,  I  believe  we  do have  some  ways  where  if  you have  a  private  Github  repo, then  you  can  keep  that  there  so you  can  show  to  like  recruiters. But  you  should  not  be  making  it  public visible  for  anyone  who  could possibly  be  taking  the  course. Yeah.  Any  other  questions? Yeah.  What's  your  policy on  notes  for  the  exam? What's  our  policy  on  notes  for  the  exam? I  believe  we'll  probably  be  doing  like one  or  two  pages  of  cheat  sheets. Yeah.  So  you'll  be  able  to bring  in  a  page  of  handwritten  notes. Other  than  that, the  exams  will  be  closed  notes. Great  question.  Yeah.  Any  other  questions? Yeah.  65%  of  the  average. Yes.  65%  on  exams  and  95%  on  everything  else. We'll  get  you  a  plus  or  a  B2b  plus.  Yeah. It's  about  a  33  average. Yeah,  it's  on  the  high  range,  right? 27033.  We  chose  the  high  range. It's  pretty  cool.  That's  nice. Yeah.  So  you  want  to  go  back? I'll  take  a  bit.  Thank  you  so  much. In  summary,  you  learned  a  lot  today. It  was  a  whirlwind.  We're  going  to  have handouts  every  single  class. If  you  don't  want  to  handout,  don't  take  one. If  you  have  a  laptop  or an  ipad,  just  use  that. If  you  don't  want  to  hand out  save  some  trees, we'll  count  the  excess  handouts and  then  we're  going  to  make  fewer  next  time. Okay.  In  summer  you  learn  six  ideas  or computer  architecture, they're  shown  on  the  screen. We  cannot  wait  for  this  course  to start  and  we'll  see  you  on  Friday.

